{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a8ea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "data = sklearn.datasets.load_diabetes()\n",
    "\n",
    "df_X=data.data\n",
    "df_y=data.target\n",
    "\n",
    "print(df_X.shape)\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f61f6cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00421986\n",
      "   0.00306441]]\n",
      "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_X = np.array(df_X)\n",
    "print(df_X)\n",
    "df_y = np.array(df_y)\n",
    "print(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5082f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faf78973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 및 파라미터 초기화\n",
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "598f7cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 정의\n",
    "def MSE(a, b):\n",
    "  mse = ((a - b) ** 2).mean() \n",
    "  return mse\n",
    "\n",
    "def loss(x, w, b, y):\n",
    "  predictions = model(x, w, b)\n",
    "  L = MSE(predictions, y)\n",
    "  return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99208d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient 기울기 계산 함수\n",
    "def gradient(X, W, b, y):\n",
    "    # N은 데이터 포인트의 개수\n",
    "    N = len(y)\n",
    "    \n",
    "    # y_pred 준비\n",
    "    y_pred = model(X, W, b)\n",
    "    \n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "        \n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "132648f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법을 위한 러닝레이트 설정\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f994cffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 6256.1203\n",
      "Iteration 20 : Loss 5913.0258\n",
      "Iteration 30 : Loss 5836.2915\n",
      "Iteration 40 : Loss 5764.8841\n",
      "Iteration 50 : Loss 5695.7178\n",
      "Iteration 60 : Loss 5628.6831\n",
      "Iteration 70 : Loss 5563.7074\n",
      "Iteration 80 : Loss 5500.7210\n",
      "Iteration 90 : Loss 5439.6566\n",
      "Iteration 100 : Loss 5380.4494\n",
      "Iteration 110 : Loss 5323.0367\n",
      "Iteration 120 : Loss 5267.3581\n",
      "Iteration 130 : Loss 5213.3552\n",
      "Iteration 140 : Loss 5160.9717\n",
      "Iteration 150 : Loss 5110.1534\n",
      "Iteration 160 : Loss 5060.8476\n",
      "Iteration 170 : Loss 5013.0039\n",
      "Iteration 180 : Loss 4966.5733\n",
      "Iteration 190 : Loss 4921.5087\n",
      "Iteration 200 : Loss 4877.7645\n",
      "Iteration 210 : Loss 4835.2968\n",
      "Iteration 220 : Loss 4794.0633\n",
      "Iteration 230 : Loss 4754.0229\n",
      "Iteration 240 : Loss 4715.1361\n",
      "Iteration 250 : Loss 4677.3648\n",
      "Iteration 260 : Loss 4640.6722\n",
      "Iteration 270 : Loss 4605.0227\n",
      "Iteration 280 : Loss 4570.3819\n",
      "Iteration 290 : Loss 4536.7168\n",
      "Iteration 300 : Loss 4503.9954\n",
      "Iteration 310 : Loss 4472.1867\n",
      "Iteration 320 : Loss 4441.2610\n",
      "Iteration 330 : Loss 4411.1894\n",
      "Iteration 340 : Loss 4381.9443\n",
      "Iteration 350 : Loss 4353.4987\n",
      "Iteration 360 : Loss 4325.8267\n",
      "Iteration 370 : Loss 4298.9034\n",
      "Iteration 380 : Loss 4272.7045\n",
      "Iteration 390 : Loss 4247.2068\n",
      "Iteration 400 : Loss 4222.3877\n",
      "Iteration 410 : Loss 4198.2255\n",
      "Iteration 420 : Loss 4174.6991\n",
      "Iteration 430 : Loss 4151.7883\n",
      "Iteration 440 : Loss 4129.4735\n",
      "Iteration 450 : Loss 4107.7358\n",
      "Iteration 460 : Loss 4086.5569\n",
      "Iteration 470 : Loss 4065.9191\n",
      "Iteration 480 : Loss 4045.8055\n",
      "Iteration 490 : Loss 4026.1994\n",
      "Iteration 500 : Loss 4007.0852\n",
      "Iteration 510 : Loss 3988.4473\n",
      "Iteration 520 : Loss 3970.2709\n",
      "Iteration 530 : Loss 3952.5418\n",
      "Iteration 540 : Loss 3935.2460\n",
      "Iteration 550 : Loss 3918.3702\n",
      "Iteration 560 : Loss 3901.9014\n",
      "Iteration 570 : Loss 3885.8272\n",
      "Iteration 580 : Loss 3870.1356\n",
      "Iteration 590 : Loss 3854.8147\n",
      "Iteration 600 : Loss 3839.8534\n",
      "Iteration 610 : Loss 3825.2409\n",
      "Iteration 620 : Loss 3810.9664\n",
      "Iteration 630 : Loss 3797.0200\n",
      "Iteration 640 : Loss 3783.3918\n",
      "Iteration 650 : Loss 3770.0722\n",
      "Iteration 660 : Loss 3757.0522\n",
      "Iteration 670 : Loss 3744.3227\n",
      "Iteration 680 : Loss 3731.8754\n",
      "Iteration 690 : Loss 3719.7017\n",
      "Iteration 700 : Loss 3707.7939\n",
      "Iteration 710 : Loss 3696.1440\n",
      "Iteration 720 : Loss 3684.7446\n",
      "Iteration 730 : Loss 3673.5885\n",
      "Iteration 740 : Loss 3662.6687\n",
      "Iteration 750 : Loss 3651.9783\n",
      "Iteration 760 : Loss 3641.5109\n",
      "Iteration 770 : Loss 3631.2601\n",
      "Iteration 780 : Loss 3621.2198\n",
      "Iteration 790 : Loss 3611.3840\n",
      "Iteration 800 : Loss 3601.7471\n",
      "Iteration 810 : Loss 3592.3034\n",
      "Iteration 820 : Loss 3583.0476\n",
      "Iteration 830 : Loss 3573.9746\n",
      "Iteration 840 : Loss 3565.0793\n",
      "Iteration 850 : Loss 3556.3568\n",
      "Iteration 860 : Loss 3547.8025\n",
      "Iteration 870 : Loss 3539.4118\n",
      "Iteration 880 : Loss 3531.1802\n",
      "Iteration 890 : Loss 3523.1036\n",
      "Iteration 900 : Loss 3515.1779\n",
      "Iteration 910 : Loss 3507.3989\n",
      "Iteration 920 : Loss 3499.7629\n",
      "Iteration 930 : Loss 3492.2661\n",
      "Iteration 940 : Loss 3484.9049\n",
      "Iteration 950 : Loss 3477.6758\n",
      "Iteration 960 : Loss 3470.5754\n",
      "Iteration 970 : Loss 3463.6003\n",
      "Iteration 980 : Loss 3456.7475\n",
      "Iteration 990 : Loss 3450.0137\n",
      "Iteration 1000 : Loss 3443.3961\n",
      "Iteration 1010 : Loss 3436.8918\n",
      "Iteration 1020 : Loss 3430.4978\n",
      "Iteration 1030 : Loss 3424.2116\n",
      "Iteration 1040 : Loss 3418.0304\n",
      "Iteration 1050 : Loss 3411.9517\n",
      "Iteration 1060 : Loss 3405.9731\n",
      "Iteration 1070 : Loss 3400.0920\n",
      "Iteration 1080 : Loss 3394.3063\n",
      "Iteration 1090 : Loss 3388.6137\n",
      "Iteration 1100 : Loss 3383.0118\n",
      "Iteration 1110 : Loss 3377.4987\n",
      "Iteration 1120 : Loss 3372.0723\n",
      "Iteration 1130 : Loss 3366.7306\n",
      "Iteration 1140 : Loss 3361.4716\n",
      "Iteration 1150 : Loss 3356.2935\n",
      "Iteration 1160 : Loss 3351.1944\n",
      "Iteration 1170 : Loss 3346.1725\n",
      "Iteration 1180 : Loss 3341.2262\n",
      "Iteration 1190 : Loss 3336.3538\n",
      "Iteration 1200 : Loss 3331.5536\n",
      "Iteration 1210 : Loss 3326.8242\n",
      "Iteration 1220 : Loss 3322.1639\n",
      "Iteration 1230 : Loss 3317.5712\n",
      "Iteration 1240 : Loss 3313.0448\n",
      "Iteration 1250 : Loss 3308.5833\n",
      "Iteration 1260 : Loss 3304.1852\n",
      "Iteration 1270 : Loss 3299.8493\n",
      "Iteration 1280 : Loss 3295.5742\n",
      "Iteration 1290 : Loss 3291.3588\n",
      "Iteration 1300 : Loss 3287.2018\n",
      "Iteration 1310 : Loss 3283.1021\n",
      "Iteration 1320 : Loss 3279.0584\n",
      "Iteration 1330 : Loss 3275.0697\n",
      "Iteration 1340 : Loss 3271.1349\n",
      "Iteration 1350 : Loss 3267.2530\n",
      "Iteration 1360 : Loss 3263.4228\n",
      "Iteration 1370 : Loss 3259.6435\n",
      "Iteration 1380 : Loss 3255.9140\n",
      "Iteration 1390 : Loss 3252.2334\n",
      "Iteration 1400 : Loss 3248.6008\n",
      "Iteration 1410 : Loss 3245.0153\n",
      "Iteration 1420 : Loss 3241.4759\n",
      "Iteration 1430 : Loss 3237.9820\n",
      "Iteration 1440 : Loss 3234.5326\n",
      "Iteration 1450 : Loss 3231.1268\n",
      "Iteration 1460 : Loss 3227.7641\n",
      "Iteration 1470 : Loss 3224.4435\n",
      "Iteration 1480 : Loss 3221.1643\n",
      "Iteration 1490 : Loss 3217.9259\n",
      "Iteration 1500 : Loss 3214.7274\n",
      "Iteration 1510 : Loss 3211.5682\n",
      "Iteration 1520 : Loss 3208.4477\n",
      "Iteration 1530 : Loss 3205.3651\n",
      "Iteration 1540 : Loss 3202.3198\n",
      "Iteration 1550 : Loss 3199.3113\n",
      "Iteration 1560 : Loss 3196.3388\n",
      "Iteration 1570 : Loss 3193.4019\n",
      "Iteration 1580 : Loss 3190.4998\n",
      "Iteration 1590 : Loss 3187.6321\n",
      "Iteration 1600 : Loss 3184.7981\n",
      "Iteration 1610 : Loss 3181.9975\n",
      "Iteration 1620 : Loss 3179.2295\n",
      "Iteration 1630 : Loss 3176.4937\n",
      "Iteration 1640 : Loss 3173.7896\n",
      "Iteration 1650 : Loss 3171.1167\n",
      "Iteration 1660 : Loss 3168.4746\n",
      "Iteration 1670 : Loss 3165.8626\n",
      "Iteration 1680 : Loss 3163.2805\n",
      "Iteration 1690 : Loss 3160.7277\n",
      "Iteration 1700 : Loss 3158.2038\n",
      "Iteration 1710 : Loss 3155.7083\n",
      "Iteration 1720 : Loss 3153.2408\n",
      "Iteration 1730 : Loss 3150.8010\n",
      "Iteration 1740 : Loss 3148.3884\n",
      "Iteration 1750 : Loss 3146.0026\n",
      "Iteration 1760 : Loss 3143.6433\n",
      "Iteration 1770 : Loss 3141.3100\n",
      "Iteration 1780 : Loss 3139.0024\n",
      "Iteration 1790 : Loss 3136.7201\n",
      "Iteration 1800 : Loss 3134.4627\n",
      "Iteration 1810 : Loss 3132.2300\n",
      "Iteration 1820 : Loss 3130.0215\n",
      "Iteration 1830 : Loss 3127.8370\n",
      "Iteration 1840 : Loss 3125.6760\n",
      "Iteration 1850 : Loss 3123.5383\n",
      "Iteration 1860 : Loss 3121.4236\n",
      "Iteration 1870 : Loss 3119.3316\n",
      "Iteration 1880 : Loss 3117.2619\n",
      "Iteration 1890 : Loss 3115.2142\n",
      "Iteration 1900 : Loss 3113.1883\n",
      "Iteration 1910 : Loss 3111.1839\n",
      "Iteration 1920 : Loss 3109.2006\n",
      "Iteration 1930 : Loss 3107.2383\n",
      "Iteration 1940 : Loss 3105.2966\n",
      "Iteration 1950 : Loss 3103.3752\n",
      "Iteration 1960 : Loss 3101.4740\n",
      "Iteration 1970 : Loss 3099.5926\n",
      "Iteration 1980 : Loss 3097.7308\n",
      "Iteration 1990 : Loss 3095.8884\n",
      "Iteration 2000 : Loss 3094.0650\n",
      "Iteration 2010 : Loss 3092.2606\n",
      "Iteration 2020 : Loss 3090.4747\n",
      "Iteration 2030 : Loss 3088.7073\n",
      "Iteration 2040 : Loss 3086.9580\n",
      "Iteration 2050 : Loss 3085.2266\n",
      "Iteration 2060 : Loss 3083.5130\n",
      "Iteration 2070 : Loss 3081.8169\n",
      "Iteration 2080 : Loss 3080.1381\n",
      "Iteration 2090 : Loss 3078.4764\n",
      "Iteration 2100 : Loss 3076.8316\n",
      "Iteration 2110 : Loss 3075.2034\n",
      "Iteration 2120 : Loss 3073.5917\n",
      "Iteration 2130 : Loss 3071.9963\n",
      "Iteration 2140 : Loss 3070.4170\n",
      "Iteration 2150 : Loss 3068.8536\n",
      "Iteration 2160 : Loss 3067.3059\n",
      "Iteration 2170 : Loss 3065.7737\n",
      "Iteration 2180 : Loss 3064.2568\n",
      "Iteration 2190 : Loss 3062.7552\n",
      "Iteration 2200 : Loss 3061.2685\n",
      "Iteration 2210 : Loss 3059.7966\n",
      "Iteration 2220 : Loss 3058.3394\n",
      "Iteration 2230 : Loss 3056.8967\n",
      "Iteration 2240 : Loss 3055.4683\n",
      "Iteration 2250 : Loss 3054.0540\n",
      "Iteration 2260 : Loss 3052.6537\n",
      "Iteration 2270 : Loss 3051.2673\n",
      "Iteration 2280 : Loss 3049.8945\n",
      "Iteration 2290 : Loss 3048.5352\n",
      "Iteration 2300 : Loss 3047.1893\n",
      "Iteration 2310 : Loss 3045.8567\n",
      "Iteration 2320 : Loss 3044.5371\n",
      "Iteration 2330 : Loss 3043.2304\n",
      "Iteration 2340 : Loss 3041.9365\n",
      "Iteration 2350 : Loss 3040.6552\n",
      "Iteration 2360 : Loss 3039.3865\n",
      "Iteration 2370 : Loss 3038.1301\n",
      "Iteration 2380 : Loss 3036.8860\n",
      "Iteration 2390 : Loss 3035.6539\n",
      "Iteration 2400 : Loss 3034.4338\n",
      "Iteration 2410 : Loss 3033.2255\n",
      "Iteration 2420 : Loss 3032.0290\n",
      "Iteration 2430 : Loss 3030.8440\n",
      "Iteration 2440 : Loss 3029.6705\n",
      "Iteration 2450 : Loss 3028.5083\n",
      "Iteration 2460 : Loss 3027.3573\n",
      "Iteration 2470 : Loss 3026.2173\n",
      "Iteration 2480 : Loss 3025.0884\n",
      "Iteration 2490 : Loss 3023.9703\n",
      "Iteration 2500 : Loss 3022.8629\n",
      "Iteration 2510 : Loss 3021.7662\n",
      "Iteration 2520 : Loss 3020.6800\n",
      "Iteration 2530 : Loss 3019.6041\n",
      "Iteration 2540 : Loss 3018.5386\n",
      "Iteration 2550 : Loss 3017.4832\n",
      "Iteration 2560 : Loss 3016.4379\n",
      "Iteration 2570 : Loss 3015.4025\n",
      "Iteration 2580 : Loss 3014.3771\n",
      "Iteration 2590 : Loss 3013.3613\n",
      "Iteration 2600 : Loss 3012.3553\n",
      "Iteration 2610 : Loss 3011.3587\n",
      "Iteration 2620 : Loss 3010.3717\n",
      "Iteration 2630 : Loss 3009.3940\n",
      "Iteration 2640 : Loss 3008.4255\n",
      "Iteration 2650 : Loss 3007.4662\n",
      "Iteration 2660 : Loss 3006.5160\n",
      "Iteration 2670 : Loss 3005.5747\n",
      "Iteration 2680 : Loss 3004.6424\n",
      "Iteration 2690 : Loss 3003.7188\n",
      "Iteration 2700 : Loss 3002.8039\n",
      "Iteration 2710 : Loss 3001.8976\n",
      "Iteration 2720 : Loss 3000.9999\n",
      "Iteration 2730 : Loss 3000.1105\n",
      "Iteration 2740 : Loss 2999.2296\n",
      "Iteration 2750 : Loss 2998.3568\n",
      "Iteration 2760 : Loss 2997.4923\n",
      "Iteration 2770 : Loss 2996.6359\n",
      "Iteration 2780 : Loss 2995.7874\n",
      "Iteration 2790 : Loss 2994.9469\n",
      "Iteration 2800 : Loss 2994.1143\n",
      "Iteration 2810 : Loss 2993.2894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2820 : Loss 2992.4722\n",
      "Iteration 2830 : Loss 2991.6627\n",
      "Iteration 2840 : Loss 2990.8606\n",
      "Iteration 2850 : Loss 2990.0660\n",
      "Iteration 2860 : Loss 2989.2789\n",
      "Iteration 2870 : Loss 2988.4990\n",
      "Iteration 2880 : Loss 2987.7263\n",
      "Iteration 2890 : Loss 2986.9609\n",
      "Iteration 2900 : Loss 2986.2025\n",
      "Iteration 2910 : Loss 2985.4511\n",
      "Iteration 2920 : Loss 2984.7067\n",
      "Iteration 2930 : Loss 2983.9692\n",
      "Iteration 2940 : Loss 2983.2385\n",
      "Iteration 2950 : Loss 2982.5145\n",
      "Iteration 2960 : Loss 2981.7972\n",
      "Iteration 2970 : Loss 2981.0866\n",
      "Iteration 2980 : Loss 2980.3825\n",
      "Iteration 2990 : Loss 2979.6848\n",
      "Iteration 3000 : Loss 2978.9936\n",
      "Iteration 3010 : Loss 2978.3087\n",
      "Iteration 3020 : Loss 2977.6301\n",
      "Iteration 3030 : Loss 2976.9578\n",
      "Iteration 3040 : Loss 2976.2916\n",
      "Iteration 3050 : Loss 2975.6315\n",
      "Iteration 3060 : Loss 2974.9775\n",
      "Iteration 3070 : Loss 2974.3294\n",
      "Iteration 3080 : Loss 2973.6873\n",
      "Iteration 3090 : Loss 2973.0510\n",
      "Iteration 3100 : Loss 2972.4206\n",
      "Iteration 3110 : Loss 2971.7959\n",
      "Iteration 3120 : Loss 2971.1769\n",
      "Iteration 3130 : Loss 2970.5636\n",
      "Iteration 3140 : Loss 2969.9558\n",
      "Iteration 3150 : Loss 2969.3536\n",
      "Iteration 3160 : Loss 2968.7568\n",
      "Iteration 3170 : Loss 2968.1655\n",
      "Iteration 3180 : Loss 2967.5795\n",
      "Iteration 3190 : Loss 2966.9989\n",
      "Iteration 3200 : Loss 2966.4235\n",
      "Iteration 3210 : Loss 2965.8534\n",
      "Iteration 3220 : Loss 2965.2884\n",
      "Iteration 3230 : Loss 2964.7285\n",
      "Iteration 3240 : Loss 2964.1737\n",
      "Iteration 3250 : Loss 2963.6239\n",
      "Iteration 3260 : Loss 2963.0790\n",
      "Iteration 3270 : Loss 2962.5391\n",
      "Iteration 3280 : Loss 2962.0041\n",
      "Iteration 3290 : Loss 2961.4738\n",
      "Iteration 3300 : Loss 2960.9484\n",
      "Iteration 3310 : Loss 2960.4277\n",
      "Iteration 3320 : Loss 2959.9117\n",
      "Iteration 3330 : Loss 2959.4003\n",
      "Iteration 3340 : Loss 2958.8935\n",
      "Iteration 3350 : Loss 2958.3912\n",
      "Iteration 3360 : Loss 2957.8935\n",
      "Iteration 3370 : Loss 2957.4002\n",
      "Iteration 3380 : Loss 2956.9113\n",
      "Iteration 3390 : Loss 2956.4269\n",
      "Iteration 3400 : Loss 2955.9467\n",
      "Iteration 3410 : Loss 2955.4709\n",
      "Iteration 3420 : Loss 2954.9993\n",
      "Iteration 3430 : Loss 2954.5319\n",
      "Iteration 3440 : Loss 2954.0687\n",
      "Iteration 3450 : Loss 2953.6096\n",
      "Iteration 3460 : Loss 2953.1546\n",
      "Iteration 3470 : Loss 2952.7037\n",
      "Iteration 3480 : Loss 2952.2567\n",
      "Iteration 3490 : Loss 2951.8138\n",
      "Iteration 3500 : Loss 2951.3748\n",
      "Iteration 3510 : Loss 2950.9397\n",
      "Iteration 3520 : Loss 2950.5084\n",
      "Iteration 3530 : Loss 2950.0810\n",
      "Iteration 3540 : Loss 2949.6574\n",
      "Iteration 3550 : Loss 2949.2375\n",
      "Iteration 3560 : Loss 2948.8213\n",
      "Iteration 3570 : Loss 2948.4088\n",
      "Iteration 3580 : Loss 2948.0000\n",
      "Iteration 3590 : Loss 2947.5948\n",
      "Iteration 3600 : Loss 2947.1932\n",
      "Iteration 3610 : Loss 2946.7951\n",
      "Iteration 3620 : Loss 2946.4005\n",
      "Iteration 3630 : Loss 2946.0094\n",
      "Iteration 3640 : Loss 2945.6217\n",
      "Iteration 3650 : Loss 2945.2374\n",
      "Iteration 3660 : Loss 2944.8566\n",
      "Iteration 3670 : Loss 2944.4790\n",
      "Iteration 3680 : Loss 2944.1048\n",
      "Iteration 3690 : Loss 2943.7339\n",
      "Iteration 3700 : Loss 2943.3662\n",
      "Iteration 3710 : Loss 2943.0018\n",
      "Iteration 3720 : Loss 2942.6405\n",
      "Iteration 3730 : Loss 2942.2824\n",
      "Iteration 3740 : Loss 2941.9275\n",
      "Iteration 3750 : Loss 2941.5756\n",
      "Iteration 3760 : Loss 2941.2268\n",
      "Iteration 3770 : Loss 2940.8811\n",
      "Iteration 3780 : Loss 2940.5384\n",
      "Iteration 3790 : Loss 2940.1986\n",
      "Iteration 3800 : Loss 2939.8619\n",
      "Iteration 3810 : Loss 2939.5280\n",
      "Iteration 3820 : Loss 2939.1971\n",
      "Iteration 3830 : Loss 2938.8690\n",
      "Iteration 3840 : Loss 2938.5438\n",
      "Iteration 3850 : Loss 2938.2214\n",
      "Iteration 3860 : Loss 2937.9019\n",
      "Iteration 3870 : Loss 2937.5850\n",
      "Iteration 3880 : Loss 2937.2710\n",
      "Iteration 3890 : Loss 2936.9596\n",
      "Iteration 3900 : Loss 2936.6510\n",
      "Iteration 3910 : Loss 2936.3450\n",
      "Iteration 3920 : Loss 2936.0416\n",
      "Iteration 3930 : Loss 2935.7409\n",
      "Iteration 3940 : Loss 2935.4428\n",
      "Iteration 3950 : Loss 2935.1472\n",
      "Iteration 3960 : Loss 2934.8542\n",
      "Iteration 3970 : Loss 2934.5637\n",
      "Iteration 3980 : Loss 2934.2757\n",
      "Iteration 3990 : Loss 2933.9902\n",
      "Iteration 4000 : Loss 2933.7072\n",
      "Iteration 4010 : Loss 2933.4266\n",
      "Iteration 4020 : Loss 2933.1483\n",
      "Iteration 4030 : Loss 2932.8725\n",
      "Iteration 4040 : Loss 2932.5991\n",
      "Iteration 4050 : Loss 2932.3279\n",
      "Iteration 4060 : Loss 2932.0591\n",
      "Iteration 4070 : Loss 2931.7926\n",
      "Iteration 4080 : Loss 2931.5284\n",
      "Iteration 4090 : Loss 2931.2664\n",
      "Iteration 4100 : Loss 2931.0067\n",
      "Iteration 4110 : Loss 2930.7492\n",
      "Iteration 4120 : Loss 2930.4939\n",
      "Iteration 4130 : Loss 2930.2407\n",
      "Iteration 4140 : Loss 2929.9897\n",
      "Iteration 4150 : Loss 2929.7409\n",
      "Iteration 4160 : Loss 2929.4941\n",
      "Iteration 4170 : Loss 2929.2495\n",
      "Iteration 4180 : Loss 2929.0069\n",
      "Iteration 4190 : Loss 2928.7664\n",
      "Iteration 4200 : Loss 2928.5279\n",
      "Iteration 4210 : Loss 2928.2915\n",
      "Iteration 4220 : Loss 2928.0570\n",
      "Iteration 4230 : Loss 2927.8246\n",
      "Iteration 4240 : Loss 2927.5941\n",
      "Iteration 4250 : Loss 2927.3655\n",
      "Iteration 4260 : Loss 2927.1389\n",
      "Iteration 4270 : Loss 2926.9141\n",
      "Iteration 4280 : Loss 2926.6913\n",
      "Iteration 4290 : Loss 2926.4704\n",
      "Iteration 4300 : Loss 2926.2513\n",
      "Iteration 4310 : Loss 2926.0340\n",
      "Iteration 4320 : Loss 2925.8186\n",
      "Iteration 4330 : Loss 2925.6050\n",
      "Iteration 4340 : Loss 2925.3932\n",
      "Iteration 4350 : Loss 2925.1831\n",
      "Iteration 4360 : Loss 2924.9749\n",
      "Iteration 4370 : Loss 2924.7683\n",
      "Iteration 4380 : Loss 2924.5635\n",
      "Iteration 4390 : Loss 2924.3604\n",
      "Iteration 4400 : Loss 2924.1590\n",
      "Iteration 4410 : Loss 2923.9593\n",
      "Iteration 4420 : Loss 2923.7612\n",
      "Iteration 4430 : Loss 2923.5648\n",
      "Iteration 4440 : Loss 2923.3701\n",
      "Iteration 4450 : Loss 2923.1769\n",
      "Iteration 4460 : Loss 2922.9854\n",
      "Iteration 4470 : Loss 2922.7954\n",
      "Iteration 4480 : Loss 2922.6070\n",
      "Iteration 4490 : Loss 2922.4202\n",
      "Iteration 4500 : Loss 2922.2350\n",
      "Iteration 4510 : Loss 2922.0513\n",
      "Iteration 4520 : Loss 2921.8691\n",
      "Iteration 4530 : Loss 2921.6884\n",
      "Iteration 4540 : Loss 2921.5092\n",
      "Iteration 4550 : Loss 2921.3314\n",
      "Iteration 4560 : Loss 2921.1552\n",
      "Iteration 4570 : Loss 2920.9804\n",
      "Iteration 4580 : Loss 2920.8070\n",
      "Iteration 4590 : Loss 2920.6351\n",
      "Iteration 4600 : Loss 2920.4646\n",
      "Iteration 4610 : Loss 2920.2955\n",
      "Iteration 4620 : Loss 2920.1277\n",
      "Iteration 4630 : Loss 2919.9614\n",
      "Iteration 4640 : Loss 2919.7964\n",
      "Iteration 4650 : Loss 2919.6328\n",
      "Iteration 4660 : Loss 2919.4705\n",
      "Iteration 4670 : Loss 2919.3095\n",
      "Iteration 4680 : Loss 2919.1499\n",
      "Iteration 4690 : Loss 2918.9916\n",
      "Iteration 4700 : Loss 2918.8345\n",
      "Iteration 4710 : Loss 2918.6788\n",
      "Iteration 4720 : Loss 2918.5243\n",
      "Iteration 4730 : Loss 2918.3710\n",
      "Iteration 4740 : Loss 2918.2191\n",
      "Iteration 4750 : Loss 2918.0683\n",
      "Iteration 4760 : Loss 2917.9188\n",
      "Iteration 4770 : Loss 2917.7705\n",
      "Iteration 4780 : Loss 2917.6234\n",
      "Iteration 4790 : Loss 2917.4775\n",
      "Iteration 4800 : Loss 2917.3327\n",
      "Iteration 4810 : Loss 2917.1892\n",
      "Iteration 4820 : Loss 2917.0468\n",
      "Iteration 4830 : Loss 2916.9055\n",
      "Iteration 4840 : Loss 2916.7654\n",
      "Iteration 4850 : Loss 2916.6265\n",
      "Iteration 4860 : Loss 2916.4886\n",
      "Iteration 4870 : Loss 2916.3519\n",
      "Iteration 4880 : Loss 2916.2162\n",
      "Iteration 4890 : Loss 2916.0817\n",
      "Iteration 4900 : Loss 2915.9482\n",
      "Iteration 4910 : Loss 2915.8158\n",
      "Iteration 4920 : Loss 2915.6845\n",
      "Iteration 4930 : Loss 2915.5542\n",
      "Iteration 4940 : Loss 2915.4250\n",
      "Iteration 4950 : Loss 2915.2968\n",
      "Iteration 4960 : Loss 2915.1696\n",
      "Iteration 4970 : Loss 2915.0435\n",
      "Iteration 4980 : Loss 2914.9183\n",
      "Iteration 4990 : Loss 2914.7942\n",
      "Iteration 5000 : Loss 2914.6710\n",
      "Iteration 5010 : Loss 2914.5489\n",
      "Iteration 5020 : Loss 2914.4277\n",
      "Iteration 5030 : Loss 2914.3074\n",
      "Iteration 5040 : Loss 2914.1881\n",
      "Iteration 5050 : Loss 2914.0698\n",
      "Iteration 5060 : Loss 2913.9524\n",
      "Iteration 5070 : Loss 2913.8359\n",
      "Iteration 5080 : Loss 2913.7204\n",
      "Iteration 5090 : Loss 2913.6058\n",
      "Iteration 5100 : Loss 2913.4921\n",
      "Iteration 5110 : Loss 2913.3792\n",
      "Iteration 5120 : Loss 2913.2673\n",
      "Iteration 5130 : Loss 2913.1563\n",
      "Iteration 5140 : Loss 2913.0461\n",
      "Iteration 5150 : Loss 2912.9368\n",
      "Iteration 5160 : Loss 2912.8283\n",
      "Iteration 5170 : Loss 2912.7208\n",
      "Iteration 5180 : Loss 2912.6140\n",
      "Iteration 5190 : Loss 2912.5081\n",
      "Iteration 5200 : Loss 2912.4030\n",
      "Iteration 5210 : Loss 2912.2988\n",
      "Iteration 5220 : Loss 2912.1953\n",
      "Iteration 5230 : Loss 2912.0927\n",
      "Iteration 5240 : Loss 2911.9909\n",
      "Iteration 5250 : Loss 2911.8899\n",
      "Iteration 5260 : Loss 2911.7896\n",
      "Iteration 5270 : Loss 2911.6902\n",
      "Iteration 5280 : Loss 2911.5915\n",
      "Iteration 5290 : Loss 2911.4936\n",
      "Iteration 5300 : Loss 2911.3964\n",
      "Iteration 5310 : Loss 2911.3000\n",
      "Iteration 5320 : Loss 2911.2044\n",
      "Iteration 5330 : Loss 2911.1095\n",
      "Iteration 5340 : Loss 2911.0153\n",
      "Iteration 5350 : Loss 2910.9219\n",
      "Iteration 5360 : Loss 2910.8291\n",
      "Iteration 5370 : Loss 2910.7371\n",
      "Iteration 5380 : Loss 2910.6458\n",
      "Iteration 5390 : Loss 2910.5553\n",
      "Iteration 5400 : Loss 2910.4654\n",
      "Iteration 5410 : Loss 2910.3762\n",
      "Iteration 5420 : Loss 2910.2877\n",
      "Iteration 5430 : Loss 2910.1998\n",
      "Iteration 5440 : Loss 2910.1127\n",
      "Iteration 5450 : Loss 2910.0262\n",
      "Iteration 5460 : Loss 2909.9404\n",
      "Iteration 5470 : Loss 2909.8552\n",
      "Iteration 5480 : Loss 2909.7707\n",
      "Iteration 5490 : Loss 2909.6868\n",
      "Iteration 5500 : Loss 2909.6036\n",
      "Iteration 5510 : Loss 2909.5210\n",
      "Iteration 5520 : Loss 2909.4390\n",
      "Iteration 5530 : Loss 2909.3577\n",
      "Iteration 5540 : Loss 2909.2770\n",
      "Iteration 5550 : Loss 2909.1969\n",
      "Iteration 5560 : Loss 2909.1174\n",
      "Iteration 5570 : Loss 2909.0385\n",
      "Iteration 5580 : Loss 2908.9602\n",
      "Iteration 5590 : Loss 2908.8825\n",
      "Iteration 5600 : Loss 2908.8054\n",
      "Iteration 5610 : Loss 2908.7289\n",
      "Iteration 5620 : Loss 2908.6529\n",
      "Iteration 5630 : Loss 2908.5776\n",
      "Iteration 5640 : Loss 2908.5028\n",
      "Iteration 5650 : Loss 2908.4285\n",
      "Iteration 5660 : Loss 2908.3548\n",
      "Iteration 5670 : Loss 2908.2817\n",
      "Iteration 5680 : Loss 2908.2091\n",
      "Iteration 5690 : Loss 2908.1371\n",
      "Iteration 5700 : Loss 2908.0656\n",
      "Iteration 5710 : Loss 2907.9947\n",
      "Iteration 5720 : Loss 2907.9242\n",
      "Iteration 5730 : Loss 2907.8543\n",
      "Iteration 5740 : Loss 2907.7850\n",
      "Iteration 5750 : Loss 2907.7161\n",
      "Iteration 5760 : Loss 2907.6478\n",
      "Iteration 5770 : Loss 2907.5799\n",
      "Iteration 5780 : Loss 2907.5126\n",
      "Iteration 5790 : Loss 2907.4458\n",
      "Iteration 5800 : Loss 2907.3794\n",
      "Iteration 5810 : Loss 2907.3136\n",
      "Iteration 5820 : Loss 2907.2483\n",
      "Iteration 5830 : Loss 2907.1834\n",
      "Iteration 5840 : Loss 2907.1190\n",
      "Iteration 5850 : Loss 2907.0551\n",
      "Iteration 5860 : Loss 2906.9917\n",
      "Iteration 5870 : Loss 2906.9287\n",
      "Iteration 5880 : Loss 2906.8662\n",
      "Iteration 5890 : Loss 2906.8042\n",
      "Iteration 5900 : Loss 2906.7426\n",
      "Iteration 5910 : Loss 2906.6814\n",
      "Iteration 5920 : Loss 2906.6207\n",
      "Iteration 5930 : Loss 2906.5605\n",
      "Iteration 5940 : Loss 2906.5007\n",
      "Iteration 5950 : Loss 2906.4414\n",
      "Iteration 5960 : Loss 2906.3824\n",
      "Iteration 5970 : Loss 2906.3239\n",
      "Iteration 5980 : Loss 2906.2659\n",
      "Iteration 5990 : Loss 2906.2082\n",
      "Iteration 6000 : Loss 2906.1510\n",
      "Iteration 6010 : Loss 2906.0942\n",
      "Iteration 6020 : Loss 2906.0378\n",
      "Iteration 6030 : Loss 2905.9818\n",
      "Iteration 6040 : Loss 2905.9262\n",
      "Iteration 6050 : Loss 2905.8711\n",
      "Iteration 6060 : Loss 2905.8163\n",
      "Iteration 6070 : Loss 2905.7619\n",
      "Iteration 6080 : Loss 2905.7079\n",
      "Iteration 6090 : Loss 2905.6543\n",
      "Iteration 6100 : Loss 2905.6011\n",
      "Iteration 6110 : Loss 2905.5483\n",
      "Iteration 6120 : Loss 2905.4959\n",
      "Iteration 6130 : Loss 2905.4438\n",
      "Iteration 6140 : Loss 2905.3921\n",
      "Iteration 6150 : Loss 2905.3408\n",
      "Iteration 6160 : Loss 2905.2898\n",
      "Iteration 6170 : Loss 2905.2392\n",
      "Iteration 6180 : Loss 2905.1890\n",
      "Iteration 6190 : Loss 2905.1391\n",
      "Iteration 6200 : Loss 2905.0896\n",
      "Iteration 6210 : Loss 2905.0405\n",
      "Iteration 6220 : Loss 2904.9917\n",
      "Iteration 6230 : Loss 2904.9432\n",
      "Iteration 6240 : Loss 2904.8951\n",
      "Iteration 6250 : Loss 2904.8473\n",
      "Iteration 6260 : Loss 2904.7999\n",
      "Iteration 6270 : Loss 2904.7528\n",
      "Iteration 6280 : Loss 2904.7060\n",
      "Iteration 6290 : Loss 2904.6596\n",
      "Iteration 6300 : Loss 2904.6135\n",
      "Iteration 6310 : Loss 2904.5677\n",
      "Iteration 6320 : Loss 2904.5223\n",
      "Iteration 6330 : Loss 2904.4771\n",
      "Iteration 6340 : Loss 2904.4323\n",
      "Iteration 6350 : Loss 2904.3878\n",
      "Iteration 6360 : Loss 2904.3436\n",
      "Iteration 6370 : Loss 2904.2997\n",
      "Iteration 6380 : Loss 2904.2561\n",
      "Iteration 6390 : Loss 2904.2129\n",
      "Iteration 6400 : Loss 2904.1699\n",
      "Iteration 6410 : Loss 2904.1272\n",
      "Iteration 6420 : Loss 2904.0849\n",
      "Iteration 6430 : Loss 2904.0428\n",
      "Iteration 6440 : Loss 2904.0010\n",
      "Iteration 6450 : Loss 2903.9595\n",
      "Iteration 6460 : Loss 2903.9183\n",
      "Iteration 6470 : Loss 2903.8774\n",
      "Iteration 6480 : Loss 2903.8368\n",
      "Iteration 6490 : Loss 2903.7964\n",
      "Iteration 6500 : Loss 2903.7563\n",
      "Iteration 6510 : Loss 2903.7166\n",
      "Iteration 6520 : Loss 2903.6770\n",
      "Iteration 6530 : Loss 2903.6378\n",
      "Iteration 6540 : Loss 2903.5988\n",
      "Iteration 6550 : Loss 2903.5601\n",
      "Iteration 6560 : Loss 2903.5216\n",
      "Iteration 6570 : Loss 2903.4835\n",
      "Iteration 6580 : Loss 2903.4455\n",
      "Iteration 6590 : Loss 2903.4079\n",
      "Iteration 6600 : Loss 2903.3705\n",
      "Iteration 6610 : Loss 2903.3333\n",
      "Iteration 6620 : Loss 2903.2964\n",
      "Iteration 6630 : Loss 2903.2598\n",
      "Iteration 6640 : Loss 2903.2234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6650 : Loss 2903.1873\n",
      "Iteration 6660 : Loss 2903.1514\n",
      "Iteration 6670 : Loss 2903.1157\n",
      "Iteration 6680 : Loss 2903.0803\n",
      "Iteration 6690 : Loss 2903.0451\n",
      "Iteration 6700 : Loss 2903.0102\n",
      "Iteration 6710 : Loss 2902.9755\n",
      "Iteration 6720 : Loss 2902.9410\n",
      "Iteration 6730 : Loss 2902.9068\n",
      "Iteration 6740 : Loss 2902.8728\n",
      "Iteration 6750 : Loss 2902.8390\n",
      "Iteration 6760 : Loss 2902.8054\n",
      "Iteration 6770 : Loss 2902.7721\n",
      "Iteration 6780 : Loss 2902.7390\n",
      "Iteration 6790 : Loss 2902.7061\n",
      "Iteration 6800 : Loss 2902.6735\n",
      "Iteration 6810 : Loss 2902.6410\n",
      "Iteration 6820 : Loss 2902.6088\n",
      "Iteration 6830 : Loss 2902.5768\n",
      "Iteration 6840 : Loss 2902.5450\n",
      "Iteration 6850 : Loss 2902.5134\n",
      "Iteration 6860 : Loss 2902.4820\n",
      "Iteration 6870 : Loss 2902.4508\n",
      "Iteration 6880 : Loss 2902.4199\n",
      "Iteration 6890 : Loss 2902.3891\n",
      "Iteration 6900 : Loss 2902.3585\n",
      "Iteration 6910 : Loss 2902.3282\n",
      "Iteration 6920 : Loss 2902.2980\n",
      "Iteration 6930 : Loss 2902.2681\n",
      "Iteration 6940 : Loss 2902.2383\n",
      "Iteration 6950 : Loss 2902.2087\n",
      "Iteration 6960 : Loss 2902.1794\n",
      "Iteration 6970 : Loss 2902.1502\n",
      "Iteration 6980 : Loss 2902.1212\n",
      "Iteration 6990 : Loss 2902.0924\n",
      "Iteration 7000 : Loss 2902.0638\n",
      "Iteration 7010 : Loss 2902.0354\n",
      "Iteration 7020 : Loss 2902.0071\n",
      "Iteration 7030 : Loss 2901.9790\n",
      "Iteration 7040 : Loss 2901.9512\n",
      "Iteration 7050 : Loss 2901.9235\n",
      "Iteration 7060 : Loss 2901.8960\n",
      "Iteration 7070 : Loss 2901.8686\n",
      "Iteration 7080 : Loss 2901.8414\n",
      "Iteration 7090 : Loss 2901.8145\n",
      "Iteration 7100 : Loss 2901.7876\n",
      "Iteration 7110 : Loss 2901.7610\n",
      "Iteration 7120 : Loss 2901.7345\n",
      "Iteration 7130 : Loss 2901.7082\n",
      "Iteration 7140 : Loss 2901.6821\n",
      "Iteration 7150 : Loss 2901.6561\n",
      "Iteration 7160 : Loss 2901.6303\n",
      "Iteration 7170 : Loss 2901.6047\n",
      "Iteration 7180 : Loss 2901.5792\n",
      "Iteration 7190 : Loss 2901.5539\n",
      "Iteration 7200 : Loss 2901.5287\n",
      "Iteration 7210 : Loss 2901.5038\n",
      "Iteration 7220 : Loss 2901.4789\n",
      "Iteration 7230 : Loss 2901.4542\n",
      "Iteration 7240 : Loss 2901.4297\n",
      "Iteration 7250 : Loss 2901.4054\n",
      "Iteration 7260 : Loss 2901.3811\n",
      "Iteration 7270 : Loss 2901.3571\n",
      "Iteration 7280 : Loss 2901.3332\n",
      "Iteration 7290 : Loss 2901.3094\n",
      "Iteration 7300 : Loss 2901.2858\n",
      "Iteration 7310 : Loss 2901.2623\n",
      "Iteration 7320 : Loss 2901.2390\n",
      "Iteration 7330 : Loss 2901.2159\n",
      "Iteration 7340 : Loss 2901.1928\n",
      "Iteration 7350 : Loss 2901.1700\n",
      "Iteration 7360 : Loss 2901.1472\n",
      "Iteration 7370 : Loss 2901.1246\n",
      "Iteration 7380 : Loss 2901.1022\n",
      "Iteration 7390 : Loss 2901.0798\n",
      "Iteration 7400 : Loss 2901.0577\n",
      "Iteration 7410 : Loss 2901.0356\n",
      "Iteration 7420 : Loss 2901.0137\n",
      "Iteration 7430 : Loss 2900.9919\n",
      "Iteration 7440 : Loss 2900.9703\n",
      "Iteration 7450 : Loss 2900.9488\n",
      "Iteration 7460 : Loss 2900.9274\n",
      "Iteration 7470 : Loss 2900.9061\n",
      "Iteration 7480 : Loss 2900.8850\n",
      "Iteration 7490 : Loss 2900.8640\n",
      "Iteration 7500 : Loss 2900.8432\n",
      "Iteration 7510 : Loss 2900.8225\n",
      "Iteration 7520 : Loss 2900.8018\n",
      "Iteration 7530 : Loss 2900.7814\n",
      "Iteration 7540 : Loss 2900.7610\n",
      "Iteration 7550 : Loss 2900.7408\n",
      "Iteration 7560 : Loss 2900.7207\n",
      "Iteration 7570 : Loss 2900.7007\n",
      "Iteration 7580 : Loss 2900.6808\n",
      "Iteration 7590 : Loss 2900.6611\n",
      "Iteration 7600 : Loss 2900.6414\n",
      "Iteration 7610 : Loss 2900.6219\n",
      "Iteration 7620 : Loss 2900.6025\n",
      "Iteration 7630 : Loss 2900.5832\n",
      "Iteration 7640 : Loss 2900.5641\n",
      "Iteration 7650 : Loss 2900.5450\n",
      "Iteration 7660 : Loss 2900.5261\n",
      "Iteration 7670 : Loss 2900.5072\n",
      "Iteration 7680 : Loss 2900.4885\n",
      "Iteration 7690 : Loss 2900.4699\n",
      "Iteration 7700 : Loss 2900.4514\n",
      "Iteration 7710 : Loss 2900.4330\n",
      "Iteration 7720 : Loss 2900.4148\n",
      "Iteration 7730 : Loss 2900.3966\n",
      "Iteration 7740 : Loss 2900.3785\n",
      "Iteration 7750 : Loss 2900.3606\n",
      "Iteration 7760 : Loss 2900.3427\n",
      "Iteration 7770 : Loss 2900.3250\n",
      "Iteration 7780 : Loss 2900.3073\n",
      "Iteration 7790 : Loss 2900.2898\n",
      "Iteration 7800 : Loss 2900.2724\n",
      "Iteration 7810 : Loss 2900.2550\n",
      "Iteration 7820 : Loss 2900.2378\n",
      "Iteration 7830 : Loss 2900.2207\n",
      "Iteration 7840 : Loss 2900.2036\n",
      "Iteration 7850 : Loss 2900.1867\n",
      "Iteration 7860 : Loss 2900.1699\n",
      "Iteration 7870 : Loss 2900.1531\n",
      "Iteration 7880 : Loss 2900.1365\n",
      "Iteration 7890 : Loss 2900.1199\n",
      "Iteration 7900 : Loss 2900.1035\n",
      "Iteration 7910 : Loss 2900.0871\n",
      "Iteration 7920 : Loss 2900.0708\n",
      "Iteration 7930 : Loss 2900.0547\n",
      "Iteration 7940 : Loss 2900.0386\n",
      "Iteration 7950 : Loss 2900.0226\n",
      "Iteration 7960 : Loss 2900.0067\n",
      "Iteration 7970 : Loss 2899.9909\n",
      "Iteration 7980 : Loss 2899.9752\n",
      "Iteration 7990 : Loss 2899.9595\n",
      "Iteration 8000 : Loss 2899.9440\n",
      "Iteration 8010 : Loss 2899.9285\n",
      "Iteration 8020 : Loss 2899.9132\n",
      "Iteration 8030 : Loss 2899.8979\n",
      "Iteration 8040 : Loss 2899.8827\n",
      "Iteration 8050 : Loss 2899.8676\n",
      "Iteration 8060 : Loss 2899.8525\n",
      "Iteration 8070 : Loss 2899.8376\n",
      "Iteration 8080 : Loss 2899.8227\n",
      "Iteration 8090 : Loss 2899.8080\n",
      "Iteration 8100 : Loss 2899.7933\n",
      "Iteration 8110 : Loss 2899.7786\n",
      "Iteration 8120 : Loss 2899.7641\n",
      "Iteration 8130 : Loss 2899.7497\n",
      "Iteration 8140 : Loss 2899.7353\n",
      "Iteration 8150 : Loss 2899.7210\n",
      "Iteration 8160 : Loss 2899.7068\n",
      "Iteration 8170 : Loss 2899.6926\n",
      "Iteration 8180 : Loss 2899.6786\n",
      "Iteration 8190 : Loss 2899.6646\n",
      "Iteration 8200 : Loss 2899.6507\n",
      "Iteration 8210 : Loss 2899.6369\n",
      "Iteration 8220 : Loss 2899.6231\n",
      "Iteration 8230 : Loss 2899.6094\n",
      "Iteration 8240 : Loss 2899.5958\n",
      "Iteration 8250 : Loss 2899.5823\n",
      "Iteration 8260 : Loss 2899.5688\n",
      "Iteration 8270 : Loss 2899.5554\n",
      "Iteration 8280 : Loss 2899.5421\n",
      "Iteration 8290 : Loss 2899.5289\n",
      "Iteration 8300 : Loss 2899.5157\n",
      "Iteration 8310 : Loss 2899.5026\n",
      "Iteration 8320 : Loss 2899.4896\n",
      "Iteration 8330 : Loss 2899.4766\n",
      "Iteration 8340 : Loss 2899.4637\n",
      "Iteration 8350 : Loss 2899.4509\n",
      "Iteration 8360 : Loss 2899.4381\n",
      "Iteration 8370 : Loss 2899.4254\n",
      "Iteration 8380 : Loss 2899.4128\n",
      "Iteration 8390 : Loss 2899.4002\n",
      "Iteration 8400 : Loss 2899.3878\n",
      "Iteration 8410 : Loss 2899.3753\n",
      "Iteration 8420 : Loss 2899.3630\n",
      "Iteration 8430 : Loss 2899.3507\n",
      "Iteration 8440 : Loss 2899.3384\n",
      "Iteration 8450 : Loss 2899.3263\n",
      "Iteration 8460 : Loss 2899.3142\n",
      "Iteration 8470 : Loss 2899.3021\n",
      "Iteration 8480 : Loss 2899.2902\n",
      "Iteration 8490 : Loss 2899.2782\n",
      "Iteration 8500 : Loss 2899.2664\n",
      "Iteration 8510 : Loss 2899.2546\n",
      "Iteration 8520 : Loss 2899.2429\n",
      "Iteration 8530 : Loss 2899.2312\n",
      "Iteration 8540 : Loss 2899.2196\n",
      "Iteration 8550 : Loss 2899.2080\n",
      "Iteration 8560 : Loss 2899.1965\n",
      "Iteration 8570 : Loss 2899.1851\n",
      "Iteration 8580 : Loss 2899.1737\n",
      "Iteration 8590 : Loss 2899.1624\n",
      "Iteration 8600 : Loss 2899.1511\n",
      "Iteration 8610 : Loss 2899.1399\n",
      "Iteration 8620 : Loss 2899.1288\n",
      "Iteration 8630 : Loss 2899.1177\n",
      "Iteration 8640 : Loss 2899.1067\n",
      "Iteration 8650 : Loss 2899.0957\n",
      "Iteration 8660 : Loss 2899.0847\n",
      "Iteration 8670 : Loss 2899.0739\n",
      "Iteration 8680 : Loss 2899.0631\n",
      "Iteration 8690 : Loss 2899.0523\n",
      "Iteration 8700 : Loss 2899.0416\n",
      "Iteration 8710 : Loss 2899.0309\n",
      "Iteration 8720 : Loss 2899.0203\n",
      "Iteration 8730 : Loss 2899.0098\n",
      "Iteration 8740 : Loss 2898.9993\n",
      "Iteration 8750 : Loss 2898.9888\n",
      "Iteration 8760 : Loss 2898.9784\n",
      "Iteration 8770 : Loss 2898.9681\n",
      "Iteration 8780 : Loss 2898.9578\n",
      "Iteration 8790 : Loss 2898.9476\n",
      "Iteration 8800 : Loss 2898.9374\n",
      "Iteration 8810 : Loss 2898.9272\n",
      "Iteration 8820 : Loss 2898.9171\n",
      "Iteration 8830 : Loss 2898.9071\n",
      "Iteration 8840 : Loss 2898.8971\n",
      "Iteration 8850 : Loss 2898.8871\n",
      "Iteration 8860 : Loss 2898.8772\n",
      "Iteration 8870 : Loss 2898.8674\n",
      "Iteration 8880 : Loss 2898.8576\n",
      "Iteration 8890 : Loss 2898.8478\n",
      "Iteration 8900 : Loss 2898.8381\n",
      "Iteration 8910 : Loss 2898.8284\n",
      "Iteration 8920 : Loss 2898.8188\n",
      "Iteration 8930 : Loss 2898.8092\n",
      "Iteration 8940 : Loss 2898.7997\n",
      "Iteration 8950 : Loss 2898.7902\n",
      "Iteration 8960 : Loss 2898.7808\n",
      "Iteration 8970 : Loss 2898.7714\n",
      "Iteration 8980 : Loss 2898.7620\n",
      "Iteration 8990 : Loss 2898.7527\n",
      "Iteration 9000 : Loss 2898.7435\n",
      "Iteration 9010 : Loss 2898.7343\n",
      "Iteration 9020 : Loss 2898.7251\n",
      "Iteration 9030 : Loss 2898.7159\n",
      "Iteration 9040 : Loss 2898.7069\n",
      "Iteration 9050 : Loss 2898.6978\n",
      "Iteration 9060 : Loss 2898.6888\n",
      "Iteration 9070 : Loss 2898.6798\n",
      "Iteration 9080 : Loss 2898.6709\n",
      "Iteration 9090 : Loss 2898.6620\n",
      "Iteration 9100 : Loss 2898.6532\n",
      "Iteration 9110 : Loss 2898.6444\n",
      "Iteration 9120 : Loss 2898.6356\n",
      "Iteration 9130 : Loss 2898.6269\n",
      "Iteration 9140 : Loss 2898.6182\n",
      "Iteration 9150 : Loss 2898.6096\n",
      "Iteration 9160 : Loss 2898.6009\n",
      "Iteration 9170 : Loss 2898.5924\n",
      "Iteration 9180 : Loss 2898.5839\n",
      "Iteration 9190 : Loss 2898.5754\n",
      "Iteration 9200 : Loss 2898.5669\n",
      "Iteration 9210 : Loss 2898.5585\n",
      "Iteration 9220 : Loss 2898.5501\n",
      "Iteration 9230 : Loss 2898.5418\n",
      "Iteration 9240 : Loss 2898.5335\n",
      "Iteration 9250 : Loss 2898.5252\n",
      "Iteration 9260 : Loss 2898.5170\n",
      "Iteration 9270 : Loss 2898.5088\n",
      "Iteration 9280 : Loss 2898.5006\n",
      "Iteration 9290 : Loss 2898.4925\n",
      "Iteration 9300 : Loss 2898.4844\n",
      "Iteration 9310 : Loss 2898.4764\n",
      "Iteration 9320 : Loss 2898.4684\n",
      "Iteration 9330 : Loss 2898.4604\n",
      "Iteration 9340 : Loss 2898.4524\n",
      "Iteration 9350 : Loss 2898.4445\n",
      "Iteration 9360 : Loss 2898.4366\n",
      "Iteration 9370 : Loss 2898.4288\n",
      "Iteration 9380 : Loss 2898.4210\n",
      "Iteration 9390 : Loss 2898.4132\n",
      "Iteration 9400 : Loss 2898.4055\n",
      "Iteration 9410 : Loss 2898.3978\n",
      "Iteration 9420 : Loss 2898.3901\n",
      "Iteration 9430 : Loss 2898.3824\n",
      "Iteration 9440 : Loss 2898.3748\n",
      "Iteration 9450 : Loss 2898.3672\n",
      "Iteration 9460 : Loss 2898.3597\n",
      "Iteration 9470 : Loss 2898.3522\n",
      "Iteration 9480 : Loss 2898.3447\n",
      "Iteration 9490 : Loss 2898.3372\n",
      "Iteration 9500 : Loss 2898.3298\n",
      "Iteration 9510 : Loss 2898.3224\n",
      "Iteration 9520 : Loss 2898.3151\n",
      "Iteration 9530 : Loss 2898.3077\n",
      "Iteration 9540 : Loss 2898.3004\n",
      "Iteration 9550 : Loss 2898.2932\n",
      "Iteration 9560 : Loss 2898.2859\n",
      "Iteration 9570 : Loss 2898.2787\n",
      "Iteration 9580 : Loss 2898.2715\n",
      "Iteration 9590 : Loss 2898.2644\n",
      "Iteration 9600 : Loss 2898.2573\n",
      "Iteration 9610 : Loss 2898.2502\n",
      "Iteration 9620 : Loss 2898.2431\n",
      "Iteration 9630 : Loss 2898.2361\n",
      "Iteration 9640 : Loss 2898.2291\n",
      "Iteration 9650 : Loss 2898.2221\n",
      "Iteration 9660 : Loss 2898.2151\n",
      "Iteration 9670 : Loss 2898.2082\n",
      "Iteration 9680 : Loss 2898.2013\n",
      "Iteration 9690 : Loss 2898.1944\n",
      "Iteration 9700 : Loss 2898.1876\n",
      "Iteration 9710 : Loss 2898.1808\n",
      "Iteration 9720 : Loss 2898.1740\n",
      "Iteration 9730 : Loss 2898.1672\n",
      "Iteration 9740 : Loss 2898.1605\n",
      "Iteration 9750 : Loss 2898.1538\n",
      "Iteration 9760 : Loss 2898.1471\n",
      "Iteration 9770 : Loss 2898.1404\n",
      "Iteration 9780 : Loss 2898.1338\n",
      "Iteration 9790 : Loss 2898.1272\n",
      "Iteration 9800 : Loss 2898.1206\n",
      "Iteration 9810 : Loss 2898.1141\n",
      "Iteration 9820 : Loss 2898.1076\n",
      "Iteration 9830 : Loss 2898.1010\n",
      "Iteration 9840 : Loss 2898.0946\n",
      "Iteration 9850 : Loss 2898.0881\n",
      "Iteration 9860 : Loss 2898.0817\n",
      "Iteration 9870 : Loss 2898.0753\n",
      "Iteration 9880 : Loss 2898.0689\n",
      "Iteration 9890 : Loss 2898.0625\n",
      "Iteration 9900 : Loss 2898.0562\n",
      "Iteration 9910 : Loss 2898.0499\n",
      "Iteration 9920 : Loss 2898.0436\n",
      "Iteration 9930 : Loss 2898.0374\n",
      "Iteration 9940 : Loss 2898.0311\n",
      "Iteration 9950 : Loss 2898.0249\n",
      "Iteration 9960 : Loss 2898.0187\n",
      "Iteration 9970 : Loss 2898.0125\n",
      "Iteration 9980 : Loss 2898.0064\n",
      "Iteration 9990 : Loss 2898.0003\n",
      "Iteration 10000 : Loss 2897.9942\n"
     ]
    }
   ],
   "source": [
    "# 경사하강법으로 로스를 줄이며 학습하기 \n",
    "losses = []\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e64dcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2880.3155063807353"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 성능 확인\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b897142a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 정답과 예측 데이터 함께 시각화 하기\n",
    "prediction = model(X_test, W, b)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_test[:, 0], y_test, label=\"true\")\n",
    "plt.scatter(X_test[:, 0], prediction, label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
